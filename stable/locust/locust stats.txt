2 workers:
Current limits:
    cpu: 100m
    memory: 128Mi

kubectl top pod -n locust
Users - SpawnRate

Idle


10 - 1 [RPS 3.4]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-845d67ddcd-d9c6s   2m           34Mi
locust-worker-65954b675b-jrwtq   1m           37Mi
locust-worker-65954b675b-nrdhz   1m           37Mi

100 - 5 [RPS 38.2]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-845d67ddcd-d9c6s   3m           34Mi
locust-worker-65954b675b-jrwtq   50m          79Mi
locust-worker-65954b675b-nrdhz   48m          80Mi

X 1000 - 50 [RPS ] ---> Pod crashed
    1 worker pod restarted
    incorrect worker count in locust
    -> from above the pod is already at half its limit with [100 - 5].
    -> this test was 10x hence failed. max supported would have been 2x


2 workers
Default limits

1000 - 50 [RPS 390.5]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   3m           32Mi
locust-worker-865659b668-c4mkt   458m         621Mi
locust-worker-865659b668-nqz4c   493m         651Mi

X after stopping test ---> Memory is not going down
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   2m           32Mi
locust-worker-865659b668-c4mkt   1m           620Mi
locust-worker-865659b668-nqz4c   1m           651Mi

5000 - 500 [RPS 600]
locust-master-754ddbccb6-v7s7w   4m           33Mi
locust-worker-865659b668-c4mkt   687m         2380Mi
locust-worker-865659b668-nqz4c   834m         2376Mi
x started seeing failure [api gw internal server error] ---> around 15%

X after stopping test ---> Memory is not going down
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   3m           33Mi
locust-worker-865659b668-c4mkt   1m           2380Mi
locust-worker-865659b668-nqz4c   1m           2350Mi