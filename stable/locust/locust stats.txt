# TELEMETRY Before start

Thu Oct 01 02:19 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   93m          2%     450Mi           2%
ip-10-80-4-92.ec2.internal    168m         2%     1011Mi          3%
ip-10-80-5-190.ec2.internal   113m         1%     513Mi           1%
ip-10-80-5-65.ec2.internal    87m          2%     513Mi           3%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-7jb22   3m           32Mi
locust-worker-585ffbd4b-9fp6n    1m           31Mi
locust-worker-585ffbd4b-h8fqc    1m           31Mi
locust-worker-585ffbd4b-nmbjp    1m           31Mi
locust-worker-585ffbd4b-q8tsk    1m           31Mi
locust-worker-585ffbd4b-tjdg4    1m           31Mi


# 500u 10s 5w [60, 61]
Thu Oct 01 02:28 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   89m          2%     569Mi           3%
ip-10-80-4-92.ec2.internal    214m         2%     1100Mi          3%
ip-10-80-5-190.ec2.internal   129m         1%     689Mi           2%
ip-10-80-5-65.ec2.internal    94m          2%     603Mi           3%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-7jb22   5m           32Mi
locust-worker-585ffbd4b-285rh    5m           113Mi
locust-worker-585ffbd4b-5btgs    5m           113Mi
locust-worker-585ffbd4b-vpt86    4m           112Mi
locust-worker-585ffbd4b-x9f85    4m           113Mi
locust-worker-585ffbd4b-xv78t    6m           113Mi

# 10Ku 50s 5w  [60, 61] ---> simulating ~50K
Thu Oct 01 02:46 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   152m         3%     2109Mi          13%
ip-10-80-4-92.ec2.internal    292m         3%     2641Mi          8%
ip-10-80-5-190.ec2.internal   276m         3%     3779Mi          11%
ip-10-80-5-65.ec2.internal    167m         4%     2145Mi          13%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-7jb22   5m           33Mi
locust-worker-585ffbd4b-285rh    82m          1659Mi
locust-worker-585ffbd4b-5btgs    73m          1659Mi
locust-worker-585ffbd4b-vpt86    71m          1659Mi
locust-worker-585ffbd4b-x9f85    67m          1659Mi
locust-worker-585ffbd4b-xv78t    81m          1659Mi


# 15Ku 50s 5w  [60, 61] ---> simulating ~75K
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   199m         4%     2928Mi          18%
ip-10-80-4-92.ec2.internal    400m         5%     3464Mi          10%
ip-10-80-5-190.ec2.internal   382m         4%     5465Mi          17%
ip-10-80-5-65.ec2.internal    186m         4%     2958Mi          18%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-m7hzr   5m           33Mi
locust-worker-585ffbd4b-4rckw    109m         2477Mi
locust-worker-585ffbd4b-brzw6    127m         2476Mi
locust-worker-585ffbd4b-jfpsv    134m         2476Mi
locust-worker-585ffbd4b-qq5sd    112m         2476Mi
locust-worker-585ffbd4b-r4gw6    110m         2476Mi


# lambda timeout
2020-10-01T17:30:24.218-04:00 START RequestId: 39ca434d-fbe9-4a61-8c5c-ad22e13ebd9a Version: $LATEST
2020-10-01T17:30:24.218-04:00 END RequestId: 39ca434d-fbe9-4a61-8c5c-ad22e13ebd9a
2020-10-01T17:30:24.218-04:00 REPORT RequestId: 39ca434d-fbe9-4a61-8c5c-ad22e13ebd9a	Duration: 60060.34 ms	Billed Duration: 60000 ms	Memory Size: 128 MB	Max Memory Used: 46 MB	
2020-10-01T17:30:24.218-04:00 2020-10-01T21:31:24.282Z 39ca434d-fbe9-4a61-8c5c-ad22e13ebd9a Task timed out after 60.06 seconds
# TELEMETRY End

# LOG Before start

Thu Oct 01 04:00 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   107m         2%     690Mi           4%
ip-10-80-4-92.ec2.internal    266m         3%     1193Mi          3%
ip-10-80-5-190.ec2.internal   110m         1%     890Mi           2%
ip-10-80-5-65.ec2.internal    83m          2%     688Mi           4%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-gz4xh   4m           32Mi
locust-worker-585ffbd4b-2vwsw    33m          201Mi
locust-worker-585ffbd4b-6zh8l    7m           201Mi
locust-worker-585ffbd4b-mcslb    132m         201Mi
locust-worker-585ffbd4b-w5mxg    9m           201Mi
locust-worker-585ffbd4b-zjb4d    102m         201Mi

# 1000u 20s 5w [60, 61] RPS ~20
Thu Oct 01 04:32 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   195m         4%     691Mi           4%
ip-10-80-4-92.ec2.internal    300m         3%     1196Mi          3%
ip-10-80-5-190.ec2.internal   194m         2%     893Mi           2%
ip-10-80-5-65.ec2.internal    154m         3%     690Mi           4%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-gz4xh   4m           32Mi
locust-worker-585ffbd4b-2vwsw    57m          205Mi
locust-worker-585ffbd4b-6zh8l    116m         205Mi
locust-worker-585ffbd4b-mcslb    45m          205Mi
locust-worker-585ffbd4b-w5mxg    76m          204Mi
locust-worker-585ffbd4b-zjb4d    135m         205Mi

# 5000u 50s 5w [60, 61] RPS ~90
Thu Oct 01 04:34 PM root@vpc-bijay-common-deployer-bijay-cd:~/locust-experiments/kubernetes(master)$ k top nodes; k top pod -n locust
NAME                          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-80-4-135.ec2.internal   466m         11%    1362Mi          8%
ip-10-80-4-92.ec2.internal    689m         8%     1866Mi          5%
ip-10-80-5-190.ec2.internal   574m         7%     2224Mi          6%
ip-10-80-5-65.ec2.internal    407m         10%    1354Mi          8%
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-5c5f6d67bd-gz4xh   5m           32Mi
locust-worker-585ffbd4b-2vwsw    539m         917Mi
locust-worker-585ffbd4b-6zh8l    297m         920Mi
locust-worker-585ffbd4b-mcslb    576m         919Mi
locust-worker-585ffbd4b-w5mxg    266m         922Mi
locust-worker-585ffbd4b-zjb4d    373m         919Mi

# 10000u 50s 5w [60, 61] RPS ~180
Issues observed with lambda [500, 504]


# Analysis

100000 devices uploading 2 times a day = 200000 uploads = 2.31 uploads/s
Log upload holding steady at 90RPS -> 5000u 50s 5w [60, 61] RPS ~90

1 tgz is around 500KB
1000 uploads corresponds to about 500MB
200000 uploads = 100GB/day = 3TB/month = $70.54/month [only for storage - at rest]

# LOG End






























1000u 100s 5w

927 uploads 16279 objects, 389.8 MB

1084 uploads 19512 objects, 467.3 MB

1072 uploads 19296 objects, 462.1 MB
1 failure -> "CatchResponseError('Log upload failed with code 504')"






1000 -> 500MB
50000 -> 25000MB





































2 workers:
Current limits:
    cpu: 100m
    memory: 128Mi

kubectl top pod -n locust
Users - SpawnRate

Idle


10 - 1 [RPS 3.4]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-845d67ddcd-d9c6s   2m           34Mi
locust-worker-65954b675b-jrwtq   1m           37Mi
locust-worker-65954b675b-nrdhz   1m           37Mi

100 - 5 [RPS 38.2]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-845d67ddcd-d9c6s   3m           34Mi
locust-worker-65954b675b-jrwtq   50m          79Mi
locust-worker-65954b675b-nrdhz   48m          80Mi

X 1000 - 50 [RPS ] ---> Pod crashed
    1 worker pod restarted
    incorrect worker count in locust
    -> from above the pod is already at half its limit with [100 - 5].
    -> this test was 10x hence failed. max supported would have been 2x


2 workers
Default limits

1000 - 50 [RPS 390.5]
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   3m           32Mi
locust-worker-865659b668-c4mkt   458m         621Mi
locust-worker-865659b668-nqz4c   493m         651Mi

X after stopping test ---> Memory is not going down
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   2m           32Mi
locust-worker-865659b668-c4mkt   1m           620Mi
locust-worker-865659b668-nqz4c   1m           651Mi

5000 - 500 [RPS 600]
locust-master-754ddbccb6-v7s7w   4m           33Mi
locust-worker-865659b668-c4mkt   687m         2380Mi
locust-worker-865659b668-nqz4c   834m         2376Mi
x started seeing failure [api gw internal server error] ---> around 15%

X after stopping test ---> Memory is not going down
NAME                             CPU(cores)   MEMORY(bytes)
locust-master-754ddbccb6-v7s7w   3m           33Mi
locust-worker-865659b668-c4mkt   1m           2380Mi
locust-worker-865659b668-nqz4c   1m           2350Mi




